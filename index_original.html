<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Latent variable models, deep generative models and the Hamming Machine</title>
<meta name="author" content="(Tammo Rukat)"/>

<link rel="stylesheet" href="file:./org_reveal_presentation/css/reveal.css"/>
<link rel="stylesheet" href="file:./org_reveal_presentation/css/theme/sky.css" id="theme"/>
<link rel="stylesheet" href="./local.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'file:./org_reveal_presentation/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1>Latent variable models, deep generative models and the Hamming Machine</h1>
<h2>Tammo Rukat</h2>
<h2>September 14, 2016</h2>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-sec-1">Latent variable models</a></li>
<li><a href="#/slide-sec-2">Deep neural networks</a></li>
<li><a href="#/slide-sec-3">The Hamming Machine</a></li>
</ul>
</div>
</div>
</section>

<section>
<section id="slide-sec-1">
<h2 id="sec-1">Latent variable models</h2>
<div class="outline-text-2" id="text-1">
</div></section>
<section id="slide-sec-1-1">
<h3 id="sec-1-1">What are latent variables?</h3>
<div class="column" style="float:left; width: 50%">
<ul>
<li data-fragment-index="1" class="fragment appear">Unobserved variables?</li>
<li data-fragment-index="2" class="fragment appear">Something like parameters?</li>
<li data-fragment-index="3" class="fragment appear">Example: A Gaussian mixture model $$ p(\mathbf{x}_i|\theta) = \prod\limits_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}_i|\mu_k,\Sigma_k) $$
  <img src="figures/EM_final5.png" alt="EM_final5.png" /></li>

</ul>
</div>
<div class="column" style="float:left; width: 40%">
<ul>
<li data-fragment-index="5" class="fragment appear">Latent variables:  \(\pi_k\) are local to every data point.</li>
<li data-fragment-index="6" class="fragment appear">Parameters \(\mu_k\) and \(\Sigma_k\) act globally.</li>

</ul>
</div>
</section>
<section id="slide-sec-1-2">
<h3 id="sec-1-2">Why use latent variable models?</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Visualise data</li>
<li data-fragment-index="2" class="fragment appear">Compress data</li>
<li data-fragment-index="3" class="fragment appear"><i>Understand</i> the data generating process</li>
<li data-fragment-index="4" class="fragment appear">Classification</li>
<li data-fragment-index="5" class="fragment appear">Feature learning</li>

</ul>
<aside class="notes">
<p>
Explain difference between supervised and unsupervised learning
</p>
<ul class="org-ul">
<li>Two main advantages (Murphy) 
<ol class="org-ol">
<li>fewer parameters
</li>
<li>learn representatoin
</li>
</ol>
</li>
</ul>

</aside>
</section>
<section id="slide-sec-1-3">
<h3 id="sec-1-3">Example of modified factor analysis: ZIFA</h3>
<div class="column" style="float:left; width: 40%">

<div class="figure">
<p><img src="figures/latent_zifa.png" alt="latent_zifa.png" width="80%" height="50%" />
</p>
</div>

<p>
single cell RNA seq gene expression data
</p>
</div>
<div class="column" style="float:left; width: 40%">

<div class="figure">
<p><img src="figures/latent_zifa_l.png" alt="latent_zifa_l.png" class="fragment (apprear) :frag_idx(2)" width="80%" height="50%" />
</p>
</div>
<span class="fragment (apprear) :frag_idx(1)"><p>
two-dimensional embedding
</p></span>
</div>
</section>
<section id="slide-sec-1-4">
<h3 id="sec-1-4">Directed graphical models</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Also known as: <i>Bayesian networks</i> or <i>causal networks</i></li>
<li data-fragment-index="2" class="fragment appear">By use of the chain rule of probability any joint probability distribution can be factorised as $$ p(x_{1,\ldots, N}) = p(x_1) p(x_2|x_1) p(x_3|x_2,x_1) \ldots p(x_N|x_{1,\ldots, N-1}) $$</li>
<li data-fragment-index="3" class="fragment appear">Graphical models represent joint distributions by making conditional independence assumptions.</li>

</ul>
<span class="fragment (apprear) :frag_idx(4)"><p>
$$ \text{height} \not\perp \text{vocab} $$
</p></span>
<span class="fragment (apprear) :frag_idx(5)"><p>
$$ \text{height} \perp \text{vocab} | \text{age} $$
</p></span>
<span class="fragment (apprear) :frag_idx(6)"><p>
$$ p(\text{height},\text{age},\text{vocabulary}) = p(\text{height}|\text{age}) p(\text{vocab}|\text{age}) p(\text{age}) $$
</p></span>

<div class="figure">
<p><img src="figures/cond_indep.png" alt="cond_indep.png" class="fragment (apprear) :frag_idx(6)" width="20%" height="50%" /> 
</p>
</div>

</section>
<section id="slide-sec-1-5">
<h3 id="sec-1-5">LVMs as graphical models</h3>
<aside class="notes">
<ul class="org-ul">
<li>What is a vector, matrix etc
</li>
<li>First show plate notation of a mixture model -&gt; draw on board!
</li>
<li>Explain conditional independence, i.e. what does a graphical model mean
</li>
<li>Expert systems, how graphical latent variable models work traditionally. Example from murphy?
</li>
</ul>

</aside>

<div class="column" style="float:left; width: 50%">
<ul>
<li data-fragment-index="1" class="fragment appear">Notation
<ul>
<li>Indices
<ul>
<li>\({n = 1\ldots N\; \text{- observations/specimens}}\)</li>
<li>\({d = 1\ldots D\; \text{- features (pixels, genes,}\ldots)}\)</li>
<li>\({l = 1\ldots L\; \text{- latent dimensions}}\)</li>
<li>\({k = 1\ldots K\; \text{- layers}}\)</li>

</ul></li>
<li>Variables
<ul>
<li>\({x_{nd}\; \text{- observations}}\)</li>
<li>\({u_{ld}\; \text{- parameters (globale variable)}}\)</li>
<li>\({z_{nl}\; \text{- latent variables (local variables)}}\)</li>

</ul></li>

</ul></li>
<li data-fragment-index="2" class="fragment appear">Another common example: Principal component analysis $$ p(\mathbf{z}_n) = \mathcal{N}(\mathbf{\mu}_0,\Sigma_0) \\ p(\mathbf{x}_n|\mathbf{z}_n,\theta) = \mathcal{N}(\mathbf{U} \mathbf{z}_n,\sigma^2\mathbf{I}) $$</li>

</ul>
</div>

<div class="column" style="float:left; width: 45%">
<ul>
<li data-fragment-index="3" class="fragment appear">Graphical model <img src="figures/pca_graph.png" alt="pca_graph.png" /></li>

</ul>
</div>
</section>
<section id="slide-sec-1-6">
<h3 id="sec-1-6">Restrict to a single observation and <i>unroll</i> in L and D</h3>

<div class="figure">
<p><img src="figures/toy_net.png" alt="toy_net.png" width="30%" height="50%" />
</p>
</div>

<p>
Most <b>neural networks</b> are latent variable models
</p>
</section>
<section id="slide-sec-1-7">
<h3 id="sec-1-7">Inference in latent variable models</h3>
<ul>
<li class="fragment roll-in">We want to choose parameters that maximise the marginal likelihood of the observed data $$ \mathcal{L}(\theta) = \sum\limits_{n=1}^N \log p(\mathbf{x}_n |\theta) = \sum\limits_{n=1}^N \log \left[ \sum\limits_{\mathbf{z}_n} p(\mathbf{x}_n, \mathbf{z}_n|\theta) \right] $$</li>
<li class="fragment roll-in">The marginal is typically hard to compute</li>
<li class="fragment roll-in">Ansatz: Compute complete data log likelhood $$ \mathcal{L}_c(\theta) = \sum\limits_{n=1}^N \log p(\mathbf{x}_n,\mathbf{z}_n|\theta) $$</li>
<li class="fragment roll-in">But we are not given the complete dataset \(\{\mathbf{X},\mathbf{Z}\}\), only incomplete data \(\mathbf{X}\).</li>
<li class="fragment roll-in">What <b>do</b> we know about \(\mathbf{Z}\)? The posterior for given parameters: \(p(\mathbf{Z}|\mathbf{X},\theta)\)</li>
<li class="fragment roll-in">Therefore use the expeceted complete data log likelihood under the posterior distribution of the latent variables for fixed parameters \(\theta_{\text{old}}\) $$ Q(\theta,\theta_{\text{old}}) = \sum\limits_{\mathbf{Z}} p(\mathbf{Z}|\mathbf{X},\theta_{\text{old}}) \log p(\mathbf{X},\mathbf{Z}|\theta) $$</li>

</ul>
</section>
<section id="slide-sec-1-8">
<h3 id="sec-1-8">Expectation Maximisation for a Gaussian mixture model</h3>
<ol>
<li>E-Step: Use current parameters \(\theta_{\text{old}}\) to calculate the expected complete data log likelihood \(Q\)</li>
<li>M-Step: Find new parameters that maximise \(Q\): \(\theta_{\text{new}} = \text{arg max} Q(\theta,\theta_{\text{old}})\)</li>

</ol>

<div class="figure">
<p><img src="figures/EM_Clustering_of_Old_Faithful_data.gif" alt="EM_Clustering_of_Old_Faithful_data.gif" width="35%" height="50%" />
</p>
</div>

<p>
Example for Gaussian mixture model: Very similar to K-means algorithm
</p>
<aside class="notes">
<ul class="org-ul">
<li>Discuss identifiability
</li>
<li>In general: inference is hard. will only find local modes in most models.
</li>
<li>Blank screen in the end -&gt; transition to deep learning
</li>
</ul>

</aside>

</section>
</section>
<section>
<section id="slide-sec-2">
<h2 id="sec-2">Deep neural networks</h2>
<div class="outline-text-2" id="text-2">
</div></section>
<section id="slide-sec-2-1">
<h3 id="sec-2-1">Deep Learning History</h3>
<aside class="notes">
<ul class="org-ul">
<li>Connectionism ppl used backprop
</li>
</ul>

</aside>
<ul>
<li class="fragment roll-in">Deep learning has a long history under many names:
<ul>
<li>cybernetics (1940-60)</li>
<li>connectonism (1980-90) [Hinton, Bengio, LeCun]</li>
<li>deep learning (since 2006)</li>

</ul></li>

</ul>

<div class="figure">
<p><img src="figures/waves.png" alt="waves.png" class="fragment (roll-in) :frag_idx(2)" />
</p>
</div>
<ul>
<li class="fragment roll-in">Why was DL abandoned in the 90s? Why does it work now?</li>

</ul>
</section>
<section id="slide-sec-2-2">
<h3 id="sec-2-2">Feed forward neural networks</h3>
<aside class="notes">
<ul class="org-ul">
<li>A red/blue/green car/truck/bird
</li>
</ul>

</aside>
<ul>
<li class="fragment roll-in">Many layers of nonlinear functions are stacked and applied sequentially to the data. $$ f^{(3)}(f^{(2)})f^{(1)}(\mathbf{x}))) $$</li>
<li class="fragment roll-in">These models are loosley inspired by neuroscience: Each unit resembles a neuron.</li>

</ul>

<div class="figure">
<p><img src="figures/deep_net.png" alt="deep_net.png" class="fragment (roll-in) :frag_idx(3)" width="45%" height="50%" />
</p>
</div>
<ul>
<li class="fragment roll-in">Each neuron receives inputs from the layer below and computes its output with some (nonlinear) activation function.</li>

</ul>
</section>
<section id="slide-sec-2-3">
<h3 id="sec-2-3">Deep Learning: Breakthrough in image classification</h3>
<aside class="notes">
<ul class="org-ul">
<li>Discuss standrad training, i.e. backprop
</li>
</ul>

</aside>
<ul>
<li>Breakthrough 2012: Reducing classification error on ImageNet by 50% [Krizhevsky, Sutskever, Hinton, 2012]
<ul>
<li>15 million labeled high-resolution images with ~22k categories</li>

</ul></li>

</ul>

<div class="figure">
<p><img src="figures/dnn.png" alt="dnn.png" />
</p>
</div>
</section>
<section id="slide-sec-2-4">
<h3 id="sec-2-4">How does this work?</h3>

<div class="figure">
<p><img src="figures/image_recognition_example.png" alt="image_recognition_example.png" width="50%" />
</p>
</div>
<ul>
<li>Deep layers learn more and more abstract concepts</li>

</ul>
</section>
<section id="slide-sec-2-5">
<h3 id="sec-2-5">Network size</h3>

<div class="figure">
<p><img src="figures/network_size.png" alt="network_size.png" />
</p>
</div>
</section>
<section id="slide-sec-2-6">
<h3 id="sec-2-6">DL success story</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Deep learning is extremely successfull in supervised learning tasks
<ul>
<li>Image recognition</li>
<li>Natural language processing.</li>

</ul></li>
<li data-fragment-index="2" class="fragment appear">Reinforcement learning: 
<ul>
<li>Atari games</li>
<li>Alpha Go.</li>

</ul></li>
<li data-fragment-index="3" class="fragment appear">Some successes in biomedical applications
<ul>
<li>Predict tissue-dependent splicing [Leung et al., 2014]</li>
<li>Learn quantitative structure activity relationships [Ma et al., 2015]</li>
<li>Predict binding sites for regulatory Genomics (DeepBind) [Park et al., 2015]</li>

</ul></li>

</ul>
</section>
<section id="slide-sec-2-7">
<h3 id="sec-2-7">Why does deep learning work?</h3>
<aside class="notes">
<ul class="org-ul">
<li>Neural networks are universal approximators.
</li>
<li>There exit 2**2**n boolen functions of n variables. For only 260 data points a neural network that implements a generic function in this class requires more bits than particles in the universe.
</li>
</ul>

</aside>
<ul>
<li data-fragment-index="1" class="fragment appear">Any smooth function can be approximated by a neural network with nonlinear outputs.</li>
<li data-fragment-index="2" class="fragment appear">How can deep neural nets possibly work so well? [Lin and Tegmark, 2016]</li>
<li data-fragment-index="3" class="fragment appear">Key aspect of <i>deep learning</i>: Layers of features are not engineered, but learned from the data.</li>

</ul>
<ol>
<li data-fragment-index="4" class="fragment appear">The world is described by symmetric, local low-order polynomials.
<ul>
<li>Physics: Maxwell, Navier-Stokes, Ising models are described by Hamiltonians of order 2-4</li>
<li>Statistics: Central limit theorem justifies normal approximation where the Hamiltonian is a 2nd order polynomial</li>

</ul></li>
<li data-fragment-index="5" class="fragment appear">The physical world has a hierarchical, Markovian structure.
<ul>
<li>E.g. images: pixels -&gt; edges at particular orientations -&gt; motifs of edges -&gt; objects</li>

</ul></li>

</ol>
</section>
<section id="slide-sec-2-8">
<h3 id="sec-2-8">Diffulties in inference of directed neural networks</h3>
<ul>
<li>Explaining away</li>

</ul>

<div class="figure">
<p><img src="figures/explaining_away.png" alt="explaining_away.png" />
</p>
</div>
<ul>
<li data-fragment-index="2" class="fragment appear">Example
<ul>
<li>\(x\): Tammo is happy</li>
<li>\(z_1\): It's sunny outside</li>
<li>\(z_2\): Paper got accepted</li>

</ul></li>
<li data-fragment-index="3" class="fragment appear">Given the evidence \(x\), the latent causes \(z_1\) and \(z_2\) are correlated. One is <i>explaining the other away</i>.</li>

</ul>
</section>
<section id="slide-sec-2-9">
<h3 id="sec-2-9">Unsupervised deep learning: Variational autoencoders</h3>
<ul>
<li data-fragment-index="1" class="fragment appear">Little progress in unsupervised deep learning.</li>
<li data-fragment-index="2" class="fragment appear">Seminal paper: Auto-Encoding Variational Bayes [Kingma and Welling, 2014]</li>
<li data-fragment-index="3" class="fragment appear">Standard varitional approach: Construct a tractable lower bound on the intractable marginal likelihood (evidence). $$ \mathcal{L}_{\text{ELBO}} = \log p(x) - D_{\text{KL}}(q_{\phi}(z|x)||p_{\theta}(z||x)) $$</li>
<li data-fragment-index="4" class="fragment appear">Rewrite $$ = \log p(x) - E_{z\sim q}\left[\log \frac{q(z|x)}{p(z,x)}p(x) \right] $$</li>
<li data-fragment-index="5" class="fragment appear">Move evidence into the expectatoin $$ = E_{z\sim q}\left[\log p(z,x) - \log q(z|x)\right] $$</li>
<li data-fragment-index="6" class="fragment appear">Standard form: $$ = E_{z\sim q} [\log p(z,x)] + H(q) $$</li>

</ul>
<ul>
<li data-fragment-index="8" class="fragment appear">Optimise the evidence lower bound (ELBO) with respect to the model parameters, i.e. find the \(q\) that maximises the lower bound and makes it as tight as possible.</li>
<li data-fragment-index="9" class="fragment appear">Mini batches of data and stochastic gradients (variance problems!)</li>

</ul>
</section>
<section id="slide-sec-2-10">
<h3 id="sec-2-10">Example - MNIST Manifold</h3>

<div class="figure">
<p><img src="figures/aevb_mnist.png" alt="aevb_mnist.png" width="40%" height="20%" />
</p>
</div>
</section>
<section id="slide-sec-2-11">
<h3 id="sec-2-11">Example - Generating digits</h3>

<div class="figure">
<p><img src="figures/latent_generation.png" alt="latent_generation.png" />
</p>
</div>
</section>
<section id="slide-sec-2-12">
<h3 id="sec-2-12">Example - Face Manifold</h3>

<div class="figure">
<p><img src="figures/face_manifold.png" alt="face_manifold.png" width="35%" height="20%" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-sec-3">
<h2 id="sec-3">The Hamming Machine</h2>
<ul>
<li data-fragment-index="1" class="fragment appear">Instead of continuous features, we want to model interactions of cateogrial traits!</li>
<li data-fragment-index="2" class="fragment appear">We want to learn hierarchical representations in order to understand the data generating process.</li>
<li data-fragment-index="3" class="fragment appear">We want to learn features that are useful for discriminative analysis.</li>

</ul>
<aside class="notes">
<ul class="org-ul">
<li>Instead of continuous features, we want to model interactions of cateogrial traits!
</li>
<li>Even Darwin was wondering (and some other dude)
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-1">
<h3 id="sec-3-1">Introducing the Hamming Machine</h3>
</section>
<section id="slide-sec-3-1-1">
<h4 id="sec-3-1-1">Notation</h4>
<ul>
<li>Indices
<ul>
<li>\({n = 1\ldots N\; \text{- observations/specimens}}\)</li>
<li>\({d = 1\ldots D\; \text{- features (e.g. pixels or genes)}}\)</li>
<li>\({l = 1\ldots L\; \text{- latent dimensions}}\)</li>
<li>\({k = 1\ldots K\; \text{- layers}}\)</li>

</ul></li>
<li>Variables
<ul>
<li>\({x_{nd}\; \text{- observations}}\)</li>
<li>\({u_{ld}\; \text{- parameters (globale variables, weights)}}\)</li>
<li>\({z_{nl}\; \text{- latent variables (local variables)}}\)</li>

</ul></li>

</ul>
<aside class="notes">
<p>
Make clear what variables are as compared to parameters. Only difference here is wrt what they are local.
</p>

</aside>
</section>
<section id="slide-sec-3-1-2">
<h4 id="sec-3-1-2">Model derivation</h4>
<ul>
<li data-fragment-index="1" class="fragment appear">Construct a probability distribution based on the hamming distance between two binary vectors, \({h(\mathbf{x},\mathbf{u})}\), and a dispersion parameter \({\lambda}\): $$ p(\mathbf{x}|\mathbf{u}) \propto \exp\left[ -\lambda \, h(\mathbf{x},\mathbf{u}) \right] $$</li>
<li data-fragment-index="2" class="fragment appear">Each observations \({\mathbf{x} }\) is generated from a subset of binary <b>codes</b>: \({\mathbf{u}_{l{=}1\ldots L}}\), selected by a vector of binary latent variables \({\mathbf{z}}\) $$ p(\mathbf{x}|\mathbf{U},\mathbf{z},\lambda) \propto \prod\limits_l p(\mathbf{x}|\mathbf{u}_l,\lambda)^{z_l} = \prod\limits_d \exp\left[- \sum_l z_l \lambda h(x_d,u_{ld}) \right]$$</li>
<li data-fragment-index="3" class="fragment appear">Normalising the likelihood for for binary observations yields a <b>logistic sigmoid</b>: $$ p(x_d = 1|\mathbf{z}, \mathbf{u}_{1\ldots L}, \lambda) = \frac{1}{1+\exp\left[-\lambda \sum\limits_l z_l (2u_{ld} - 1) \right]} = \sigma\left[-\lambda \sum_l z_l \tilde{u}_{ld} \right]$$</li>
<li data-fragment-index="4" class="fragment appear">We defined the mapping from \({\{0,1\}}\) to \({\{{-}1,1\}\,}\): \(\;\;{\tilde{u} = 2u{-}1}\)</li>

</ul>
<aside class="notes">
<ul class="org-ul">
<li>We use the tilde mapping throughout
</li>
<li>This migh be a bit unconventional
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-1-3">
<h4 id="sec-3-1-3">Graphical model representation</h4>
<ul>
<li>$$ p(\mathbf{x}_{n}|\mathbf{z}_n,\mathbf{U},\lambda) = \prod_d \text{Ber}  \left( x_{nd} |\sigma \left[ \lambda \sum\limits_{l=1}^L z_{ln} \tilde{u}_{ld}  \right] \right)$$</li>

</ul>


<div class="figure">
<p><img src="single_layer_network.png" alt="single_layer_network.png" />
</p>
</div>
<aside class="notes">
<ul class="org-ul">
<li>This is just a sigmoid belief net, but two differences:
<ol class="org-ol">
<li>binary parameters
</li>
<li>parameters are random variables
</li>
</ol>
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-1-4">
<h4 id="sec-3-1-4">Toy example</h4>

<div class="figure">
<p><img src="./figures/sampler_002.png" alt="sampler_002.png" />
</p>
</div>
<aside class="notes">
<ul class="org-ul">
<li>We do essentially random scan Gibbs sampling, as described later
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-1-5">
<h4 id="sec-3-1-5">Toy example</h4>

<div class="figure">
<p><img src="./figures/animation.gif" alt="animation.gif" />
</p>
</div>
</section>
<section id="slide-sec-3-2">
<h3 id="sec-3-2">Synthetic example: Calculator digits</h3>

<div class="figure">
<p><img src="./figures/digits.png" alt="digits.png" width="70%" height="50%" />
</p>
</div>
<ul>
<li>Each digit is composed of a subset of 7 distinct bars.</li>

</ul>
</section>
<section id="slide-sec-3-2-1">
<h4 id="sec-3-2-1">Noiseless calculator digits</h4>
<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/nonsparse_noisefree_data.png" alt="nonsparse_noisefree_data.png" />
compressed data
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/nonsparse_noisefreeU0.png" alt="nonsparse_noisefreeU0.png" />
inferred codes
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/codes_recon_new.png" alt="codes_recon_new.png" />
uncompressed inferred codes
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/nonsparse_noisefreeZ0.png" alt="nonsparse_noisefreeZ0.png" />
inferred latent variables
</p>
</div>

<div class="column" style="float:left; width: 50%">
<ul class="fragment appear">
<li><b>What about 3, 8 and 9?</b>
<ul>
<li>\({``7 + 2 + 5 = 3"}\)</li>
<li>\({``7 + 2 + 5 + 6 + 1 = 3"}\)</li>

</ul></li>

</ul>
</div>

</section>
<section id="slide-sec-3-2-2">
<h4 id="sec-3-2-2">Other <i>perfect</i> solutions</h4>
<div class="column" style="float:left; width: 40%">

<div class="figure">
<p><img src="./figures/snas714.png" alt="snas714.png" width="70%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 40%">

<div class="figure">
<p><img src="./figures/snas715.png" alt="snas715.png" width="70%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 8%">

<div class="figure">
<p><img src="./figures/cbar.png" alt="cbar.png" width="80%" height="20%" />
</p>
</div>
</div>


</section>
<section id="slide-sec-3-2-3">
<h4 id="sec-3-2-3">Reconstruction Error</h4>

<div class="figure">
<p><img src="calc_dist.png" alt="calc_dist.png" width="90%" height="20%" />
</p>
</div>

</section>
<section id="slide-sec-3-2-4">
<h4 id="sec-3-2-4">Denoising</h4>
<div class="column" style="float:left; width: 40%">
<ul>
<li>Calculator digits with 10% noise.</li>

</ul>
</div>

<div class="column" style="float:left; width: 100%">
</div>


<div class="column" style="float:left; width: 25%">
<p>
<img src="figures/recon_example32.png" alt="recon_example32.png" width="80%" height="20%" />
Denoised digits
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="figures/recon_example42.png" alt="recon_example42.png" width="80%" height="20%" />
Denoised digits
</p>
</div>


<div class="column" style="float:left; width: 30%">
<p>
<img src="figures/recon_example_snas.png" alt="recon_example_snas.png" width="80%" height="20%" />
Corresponding codes
</p>
</div>

</section>
<section id="slide-sec-3-3">
<h3 id="sec-3-3">The multi-layer Hamming Machine</h3>

<div class="figure">
<p><img src="twolayer_hm.png" alt="twolayer_hm.png" />
</p>
</div>

<p>
The joint density factorises in terms of the form p(layer|parents)
</p>

<p>
With \({\mathbf{z}^{[0]}_n = \mathbf{x}_n}\) and \({L^{[0]} = D}\), that is
$$  p(\mathbf{Z}^{[0:K]},\mathbf{U}^{[1:K]},\lambda) = 
  p(\mathbf{Z}^{[K]}) \prod_{k=0}^{K-1} p(\mathbf{Z}^{[k]}|\mathbf{Z}^{[k{+}1]},\mathbf{U}^{[k{+}1]},\lambda^{[k{+}1]})\, p(\mathbf{U}^{[k{+}1]})\, p(\lambda^{[k{+}1]}) 
$$
</p>
</section>
<section id="slide-sec-3-4">
<h3 id="sec-3-4">Inference and learning</h3>
<div class="outline-text-3" id="text-3-4">
</div></section>
<section id="slide-sec-3-4-1">
<h4 id="sec-3-4-1">Abbrevations</h4>
<ul>
<li><i>Observation count matrix</i>: $$ a_{nd} = \tilde{x}_{nd} \sum\limits_{l = 1}^{M} z_{ln} \tilde{u}_{ld} $$ $$ a_{nd} \in \{-L,\ldots,-1,0,1,\ldots,L \} $$</li>
<li>Enables us to write the likelihood: $$ \mathcal{L}(\mathbf{U},\mathbf{Z},\lambda) = \prod_{n,d} \sigma \left[ \lambda a_{nd} \right] $$</li>

</ul>
</section>
<section id="slide-sec-3-4-2">
<h4 id="sec-3-4-2">Random scann Gibbs sampling - conditional</h4>
<ul>
<li>Full conditionals $$  p(u_{ld}=1|\text{rest}) = \sigma \left[-\tilde{u}_{ld} \sum\limits_n \left\{ \gamma_{\lambda}(a_{nd}) - \gamma_{\lambda}(a_{nd} -  \tilde{u}_{ld} \,\tilde{x}_{nd} (\tilde{z}_{nl}+1) )\right\} \right] $$ $$ p(z_{nl}{=}1|\text{rest}) = \sigma\left[ -\tilde{z}_{ln} \sum\limits_d \left\{ \gamma_{\lambda}\left(a_{nd}\right) - \gamma_{\lambda}\left(a_{nd}-\tilde{z}_{ln}\,\tilde{x}_{nd}\,\tilde{u}_{ld}\right) \right\} \right] $$</li>
<li>Multilayer conditionals $$ p(z^{[k]}_{nl}{=}1) = \sigma\left[-\tilde{z}_{nl} \sum\limits_{d} \left\{ \log \left( 1 + \exp \left[ -\lambda a_{nd} \right] \right)
    -\log\left( 1 + \exp\left[ -\lambda (a_{nd} - \tilde{z}_{nl} \tilde{x}_{nd} \tilde{u}_{ld} \right) \right] \right)  \right. 
  \left. + \lambda^{[k+1]} \sum\limits_{l^{[k+1]}} \tilde{u}^{[k+1]}_{l^{[k+1]}l}\; z_{nl}^{[k+1]} \right] $$</li>
<li>Precomputed quantities $$  \gamma_{\lambda}(l) = \log(1+e^{-\lambda l}) $$</li>

</ul>
<aside class="notes">
<ul class="org-ul">
<li>Discuss random scan Gibbs sampling compared to other scan strategies
</li>
</ul>

</aside>

</section>
<section id="slide-sec-3-4-3">
<h4 id="sec-3-4-3">The modified metropolised Gibbs sampler</h4>
<ul>
<li data-fragment-index="1" class="fragment appear">Instead of drawing from the full conditional we always propose a value \({y'}\) that is different from the current value \({y}\), i.e. \({y' = 1-y}\).</li>
<li data-fragment-index="2" class="fragment appear">The proposal distribution then takes the form $$ q(y'|y\neq y') = 1 = \frac{p(y'|\text{rest})}{1-p(y|\text{rest})} $$</li>
<li data-fragment-index="3" class="fragment appear">And the Hasting acceptance ratio, equal the mutation probability and is given by $$ p_{\text{mutation}}^{\text{modified}} = \frac{p(\mathbf{y}')q(y|y')}{p(\mathbf{y})q(y'|y)} = \frac{p(y'|\text{rest})}{1-p(y'|\text{rest})} $$</li>
<li data-fragment-index="4" class="fragment appear">The Gibbs mutation probability is given by $$ p_{\text{mutation}}^{\text{Gibbs}} = p(y'|\text{rest}) $$</li>
<li data-fragment-index="5" class="fragment appear">And therefore the modified sampler has a <b>higher mutation probability</b> $$  p_{\text{mutation}}^{\text{modified}} > p_{\text{mutation}}^{\text{Gibbs}} $$</li>

</ul>
</section>
<section id="slide-sec-3-4-4">
<h4 id="sec-3-4-4">Alternative sampling schemes</h4>
<ol>
<li data-fragment-index="1" class="fragment appear">Forward-filtering backward-sampling for joint updates across layers
<ul>
<li>Using coniditional independence properties, like for hidden Markov models.</li>

</ul></li>
<li data-fragment-index="2" class="fragment appear">Layer-wise training
<ul>
<li>Start from the layer closest to the data</li>
<li>Train until convergence</li>
<li><i>Turn on</i> layer below</li>

</ul></li>
<li data-fragment-index="3" class="fragment appear">Simulated reheating
<ul>
<li>After convergence, reheat the system by means of the dispersion parameter \(\lambda\).</li>
<li>Sample at fixed high temperature</li>
<li>Converge back to equilibrium temperature</li>

</ul></li>
<li data-fragment-index="4" class="fragment appear">Parallel tempering
<ul>
<li>Swapping states between chains is extremely unlikely</li>

</ul></li>

</ol>
</section>
<section id="slide-sec-3-4-5">
<h4 id="sec-3-4-5">Alternative sampling schemes: Results</h4>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./figures/comparsion_overall.png" alt="comparsion_overall.png" width="65%" height="20%" />
</p>
</div>

<p>
Joint \({p(\mathbf{X},\mathbf{Z}_1,\mathbf{Z}_2|\mathbf{U},\lambda_0)}\)
</p>
</div>

<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./figures/comparsion_layer1.png" alt="comparsion_layer1.png" width="65%" height="20%" />
</p>
</div>

<p>
Data layer \({p(\mathbf{X}|\mathbf{Z}_1,\mathbf{U},\lambda_1)}\)
</p>
</div>

<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./figures/comparsion_layer2.png" alt="comparsion_layer2.png" width="65%" height="20%" />
</p>
</div>

<p>
Data layer \({p(\mathbf{Z}_1|\mathbf{Z}_2,\mathbf{U},\lambda_2)}\)
</p>
</div>

<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./figures/comparsion_layer3.png" alt="comparsion_layer3.png" width="65%" height="20%" />
</p>
</div>

<p>
Data layer \({p(\mathbf{Z}_2|\mathbf{Z}_3,\mathbf{U},\lambda_3)}\)
</p>
</div>

<aside class="notes">
<ul class="org-ul">
<li>Maybe explain the Lambda bug
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-5">
<h3 id="sec-3-5">Bernoulli priors</h3>
<div class="outline-text-3" id="text-3-5">
</div></section>
<section id="slide-sec-3-5-1">
<h4 id="sec-3-5-1">Effect on the conditionals</h4>
<ul>
<li>A Bernoulli prior on a single code unit \({u_{ld}}\):</li>

</ul>
<p>
$$ p(u_{ld}=1|\text{rest}) = \sigma \left[\color{red}{ \log\left(  \frac{ p(u_{ld}) }{ 1 - p(u_{ld}) } \right)} - \tilde{u}_{ld} \sum\limits_n \left\{ \gamma_{\lambda}(a_{nd}) - \gamma_{\lambda}(a_{nd} - \tilde{u}_{ld}\,\tilde{x}_{nd} (\tilde{s}_{mn} + 1))\right\} \right] $$
</p>
</section>
<section id="slide-sec-3-5-2">
<h4 id="sec-3-5-2">Types of priors</h4>
<div class="column" style="float:left; width: 55%">
<ol>
<li>Independent Bernoulli prior on every single code unit \({u_{md}}\)</li>
<li>Bernoulli prior controlling the number of 1s in every code. q is the ratio of 1s in code to 1s in data.</li>

</ol>
</div>

<div class="column" style="float:left; width: 100%">
<p>
E.g. <i>step-exp prior</i>
</p>
</div>

<p>
<img src="figures/prior.png" alt="prior.png" />
$$  p(u = 1) = \tfrac{1}{2} \mathrm{H}( 1 - q ) + \tfrac{1}{2} \mathrm{H}(q-1) e^{-a(q-1)} $$
</p>
</section>
<section id="slide-sec-3-5-3">
<h4 id="sec-3-5-3">Effect of the prior for synthetic data - flat prior (old example)</h4>

<div class="figure">
<p><img src="figures/a4_10_5.gif" alt="a4_10_5.gif" />
</p>
</div>
</section>
<section id="slide-sec-3-5-4">
<h4 id="sec-3-5-4">Effect of the prior for synthetic data - step exp sparsity prior</h4>

<div class="figure">
<p><img src="figures/a4_10_5_sparse.gif" alt="a4_10_5_sparse.gif" />
</p>
</div>
</section>
<section id="slide-sec-3-6">
<h3 id="sec-3-6">Application: Cancer mutational landscapes</h3>
<div class="outline-text-3" id="text-3-6">
</div></section>
<section id="slide-sec-3-6-1">
<h4 id="sec-3-6-1">TCGA data</h4>
<p>
127 specific cancer-associated genes are taken into account with specimens from four primary tissues
</p>
<ul>
<li>Acute myeloid leukemia &#x2014; AML (N=170)</li>
<li>Bladder urothelial carcinoma &#x2014; BLCA (N=97)</li>
<li>Colon adenocarcinoma &#x2014; COAD (N=155)</li>
<li>Uterine corpus endometrioid carcinoma &#x2014; UCEC (N=247)</li>

</ul>
</section>
<section id="slide-sec-3-6-2">
<h4 id="sec-3-6-2">Reference model: binary PCA</h4>
<p>
$$ p(\mathbf{z}_{n}) = \mathcal{N}(0,\sigma^2\,\mathbb{I}) $$
$$ p(x_{nd}=1|\mathbf{z}_n,\theta) = \sigma\left[ \tilde{\mathbf{u}}^T_d \mathbf{z}_n \right] $$
$$ u_{ld}, z_{nl} \in \mathbb{R} $$
</p>
<div class="column" style="float:left; width: 50%">
<p>
<img src="./figures/pca_2d_z_scatter_0.png" alt="pca_2d_z_scatter_0.png" />
Latent embedding
</p>
</div>

<div class="column" style="float:left; width: 50%">
<p>
<img src="./figures/pca_2d_u_scatter_0_zoomed.png" alt="pca_2d_u_scatter_0_zoomed.png" />
Principal components
</p>
</div>
</section>
<section id="slide-sec-3-6-3">
<h4 id="sec-3-6-3">Two-layer Hamming Machine</h4>
<ul>
<li>L = [15,4]</li>
<li>The data 95% zeros</li>

</ul>
<aside class="notes">
<ul class="org-ul">
<li>Architecture is [15,4]
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-6-4">
<h4 id="sec-3-6-4">Two-layer Hamming Machine &#x2013; first hidden layer</h4>
<div class="column" style="float:left; width: 70%">

<div class="column" style="float:left; width: 22%">

<div class="figure">
<p><img src="./figures/latent_hm0.png" alt="latent_hm0.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 22%">

<div class="figure">
<p><img src="./figures/latent_hm2.png" alt="latent_hm2.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 22%">

<div class="figure">
<p><img src="./figures/latent_hm1.png" alt="latent_hm1.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 22%">

<div class="figure">
<p><img src="./figures/latent_hm3.png" alt="latent_hm3.png" width="100%" height="20%" />
</p>
</div>
</div>

<ul>
<li class="fragment appear">Code \({z_{14}}\): NAV3 and APC are known to co-occur in various cancers. The code is active in almost all specimens</li>
<li class="fragment appear">Code \({z_{11}}\): MLL2/3 is associated to AML (<i>MML &#x2013; myeloid/lymphoid leukemia</i>)</li>
<li class="fragment appear">Code \({z_{9}}\): NFE2L2 (aka NrF2) encodes a transcription factor that is involved in the regulation inflammatory response</li>

</ul>
</div>

<div class="column" style="float:left; width: 28%">

<div class="figure">
<p><img src="./figures/snas_0.png" alt="snas_0.png" width="65%" height="20%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-3-6-5">
<h4 id="sec-3-6-5">Two-layer Hamming Machine &#x2013; second hidden layer</h4>
<div class="column" style="float:left; width: 72%">

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/latent_hm0_0.png" alt="latent_hm0_0.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/latent_hm2_0.png" alt="latent_hm2_0.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/latent_hm1_0.png" alt="latent_hm1_0.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/latent_hm3_0.png" alt="latent_hm3_0.png" width="100%" height="20%" />
</p>
</div>
</div>

<ul class="fragment appear">
<li>PTEN and PIK3 affect the cell-cycle regulating PIK3/AKT/mTOR pathway.</li>
<li>FLT3 is involved in regulation of hematopoiesis.</li>

</ul>
</div>

<div class="column" style="float:left; width: 28%">

<div class="figure">
<p><img src="./figures/snas_1.png" alt="snas_1.png" width="75%" height="20%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-3-6-6">
<h4 id="sec-3-6-6">PCA</h4>
<div class="column" style="float:left; width: 50%">
<p>
<img src="./figures/pca_0.png" alt="pca_0.png" />
First hidden layer (L=15)
</p>
</div>
<div class="column" style="float:left; width: 50%">
<p>
<img src="./figures/pca_1.png" alt="pca_1.png" />
Second hidden layer (L=4)
</p>
</div>
<aside class="notes">
<ul class="org-ul">
<li>Could do a another layer with 2 dimension and plot these(!)
</li>
</ul>

</aside>
</section>
<section id="slide-sec-3-6-7">
<h4 id="sec-3-6-7">tSNE</h4>
<div class="column" style="float:left; width: 50%">
<p>
<img src="./figures/tsne_0.png" alt="tsne_0.png" width="130%" height="100%" />
First hidden layer (L=15)
</p>
</div>
<div class="column" style="float:left; width: 50%">
<p>
<img src="./figures/tsne_1.png" alt="tsne_1.png" width="130%" height="1000%" />
Second hidden layer (L=4)
</p>
</div>
</section>
<section id="slide-sec-3-6-8">
<h4 id="sec-3-6-8">Single layer HM</h4>
<div class="column" style="float:left; width: 72%">

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/single_layer0.png" alt="single_layer0.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/single_layer1.png" alt="single_layer1.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/single_layer2.png" alt="single_layer2.png" width="100%" height="20%" />
</p>
</div>
</div>

<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./figures/single_layer3.png" alt="single_layer3.png" width="100%" height="20%" />
</p>
</div>
</div>
</div>

<div class="column" style="float:left; width: 28%">

<div class="figure">
<p><img src="./figures/snas_single.png" alt="snas_single.png" width="75%" height="20%" />
</p>
</div>
</div>
</section>
<section id="slide-sec-3-7">
<h3 id="sec-3-7">Model modification: The sparse Hamming Machine</h3>
<div class="outline-text-3" id="text-3-7">
</div></section>
<section id="slide-sec-3-7-1">
<h4 id="sec-3-7-1">Motivation</h4>
<ul>
<li>The <i>problem</i>: Every code has to vote on every feature. If a code believes that certain features appear together, than it necessarily provides the same evidence for all other features to no appear.</li>
<li>This may not reflect the generative process that we wish to capture.</li>
<li>Proposed modification: $$ \tilde{u} \in \{-1,1\} \;\; \rightarrow \;\; \tilde{u} \in \{-1,0,1\} $$</li>
<li>This yields the full conditional: $$   p(\tilde{u}_{ld}|\text{rest}) =
   \text{Cat} \left( \underset{\tilde{u}' \in \{-1,0,1\}}{\mathcal{S}} \left[ - \sum\limits_n \gamma_{\lambda}(a_{nd,\tilde{u}'}) \right] \right) $$</li>

</ul>
</section>
<section id="slide-sec-3-7-2">
<h4 id="sec-3-7-2">Synthetic example</h4>
<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/nonsparse_noisy_newdata.png" alt="nonsparse_noisy_newdata.png" />
compressed data
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/calc_digits_codesviva.png" alt="calc_digits_codesviva.png" />
inferred codes
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/calc_digits_sparse_codes_reconviva.png" alt="calc_digits_sparse_codes_reconviva.png" />
reconstruction of codes
</p>
</div>

<div class="column" style="float:left; width: 25%">
<p>
<img src="./figures/nonsparse_noisy_newZ0.png" alt="nonsparse_noisy_newZ0.png" />
latent variables
</p>

</div>

<div class="column" style="float:left; width: 20%">
<p>
<img src="./figures/calc_digits_sparse_codes_reconviva_3.png" alt="calc_digits_sparse_codes_reconviva_3.png" />
codes for \({L{=}3}\)
</p>
</div>

<div class="column" style="float:left; width: 20%">
<p>
<img src="./figures/rgb_simplex.png" alt="rgb_simplex.png" />
color legend
</p>
</div>

<div class="column" style="float:left; width: 60%">

<ul>
<li>Latent representation are sparser than for the traditional HM</li>
<li>The <i>obvious</i> single-bar representation takes L=14 codes.</li>

</ul>
</div>

</section>
<section id="slide-sec-3-7-3">
<h4 id="sec-3-7-3">Reconstruction error</h4>

<div class="figure">
<p><img src="figures/distr.png" alt="distr.png" width="80%" height="50%" />
</p>
</div>
</section>
<section id="slide-sec-3-8">
<h3 id="sec-3-8">Future work</h3>
<div class="outline-text-3" id="text-3-8">
</div></section>
<section id="slide-sec-3-8-1">
<h4 id="sec-3-8-1">The minimal Hamming Machine</h4>
<ul>
<li data-fragment-index="1" class="fragment appear">A very intuitive way of combining codes to generate observations. $$  p(x_{nd}=1|\mathbf{u}_d,\mathbf{z}_n) \propto \exp\left[{\lambda\, h(x,\min(\mathbf{u}^T\mathbf{z},1))} \right] $$</li>
<li data-fragment-index="2" class="fragment appear">The binomial parameter for node \(x_{nd}\) takes one of only 2 possible values, \({\sigma(\pm \lambda)}\).
<ul>
<li>It equals \({\sigma(+\lambda)}\) if a single pair of its parent variables is <i>turned on</i>, \((z_{nl},u_{ld}) = (1,1)\), indepedent of the value of all other parents.</li>
<li>It equald \({-\lambda}\) if all parents are <i>turned off</i>.</li>

</ul></li>

</ul>
</section>
<section id="slide-sec-3-8-2">
<h4 id="sec-3-8-2">Scalable inference with mean field fixed point equations</h4>
<ul>
<li data-fragment-index="1" class="fragment appear">Break the <i>explaining away</i> dependency between the latent variables. $$ p(\mathbf{x}|\mathbf{z},\mathbf{u}) \approx \prod\limits_{d,l} \sigma\left[ \lambda x_{d} z_l \tilde{u}_{ld}  \right] $$</li>
<li data-fragment-index="2" class="fragment appear">Iterate through all \(z_{nl}\) and \(u_{ld}\) and optimise every single one $$   \sum\limits_d \tilde{x}_{nd} \tilde{u}_{ld} + \sum\limits_{l^{[2]}} z^{[2]}_{nl} \tilde{u}^{[2]}_{ld} > 0 \; \rightarrow z_{nl} = 1,\; \text{else}\; z_{nl}=0 $$ $$  \sum\limits_n \tilde{x}_{nd} z_{nl} > 0 \; \rightarrow u_{ld} = 1,\; \text{else}\; u_{ld}=0 $$</li>
<li data-fragment-index="3" class="fragment appear">This will converge very quickly but depend heavily on the intial conditions.</li>
<li data-fragment-index="4" class="fragment appear">More sophisticated variational inference?</li>

</ul>
</section>
<section id="slide-sec-3-8-3">
<h4 id="sec-3-8-3">Prior knowledge and classification</h4>
<ul>
<li data-fragment-index="1" class="fragment appear">Prior kowledge can easily be incorporated for features and for the latent representations.
<ul>
<li>E.g. the tumour labels can be one-hot latent states</li>
<li>Different layers can correspond to different phenotypes. For instance:
<ul>
<li>The tissue type (e.g. epithelial)</li>
<li>The actual tissue of origin (colon)</li>

</ul></li>

</ul></li>
<li data-fragment-index="2" class="fragment appear">This can be extended to a generative classifier.</li>

</ul>
</section>
<section id="slide-sec-3-8-4">
<h4 id="sec-3-8-4">Further areas of application</h4>
<ul>
<li data-fragment-index="1" class="fragment appear">Network clustering
<ul>
<li>Adjacency matrices as input</li>
<li>E.g. brain networks from EEG/fMRI for different timepoints or different individuals.</li>

</ul></li>
<li data-fragment-index="2" class="fragment appear">Single cell expression data.
<ul>
<li>Binarised data for multivariate co-expression analysis.</li>

</ul></li>
<li data-fragment-index="3" class="fragment appear">Molecular fingerprints</li>

</ul>
</section>
<section id="slide-sec-3-9">
<h3 id="sec-3-9">Appendix A: MLE of Lambda</h3>

<div class="figure">
<p><img src="figures/pancan_lambda.png" alt="pancan_lambda.png" />
</p>
</div>
<ul>
<li>Maximum likelihood estimates for lambda under variation of the layer size in a single-layer HM on TCGA mutation profiles</li>

</ul>
</section>
<section id="slide-sec-3-10">
<h3 id="sec-3-10">Appendix B: Further example &#x2013; MNIST</h3>
<ul>
<li>200 images of the units 2, 7, 9</li>
<li>Two hidden layers, with 30 and 6 units respectively.</li>

</ul>
</section>
<section id="slide-sec-3-10-1">
<h4 id="sec-3-10-1">Sampling</h4>

<div class="figure">
<p><img src="figures/mnist_sampler.png" alt="mnist_sampler.png" />
</p>
</div>
</section>
<section id="slide-sec-3-10-2">
<h4 id="sec-3-10-2">Reconstructions</h4>
<p>
From the corresponding representations in layer 1 (left) and layer 2 (right)
</p>

<p>
<a href="figures/recon_1.png"><img src="./figures/recon_2.png" alt="recon_2.png" /></a>
<img src="figures/recon_2.png" alt="recon_2.png" />
</p>
</section>
<section id="slide-sec-3-10-3">
<h4 id="sec-3-10-3">Codes</h4>

<div class="figure">
<p><img src="figures/snb_small_1.png" alt="snb_small_1.png" />
</p>
</div>
</section>
<section id="slide-sec-3-10-4">
<h4 id="sec-3-10-4">Codes</h4>

<div class="figure">
<p><img src="figures/snb_small_2.png" alt="snb_small_2.png" />
</p>
</div>
</section>
</section>
</div>
</div>

<script src="file:./org_reveal_presentation/lib/js/head.min.js"></script>
<script src="file:./org_reveal_presentation/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: true,
rollingLinks: true,
keyboard: true,
overview: true,
width: 1920,
height: 1080,
margin: 0.15,
minScale: 0.50,
maxScale: 2.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'cube', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'file:./org_reveal_presentation/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'file:./org_reveal_presentation/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'file:./org_reveal_presentation/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'file:./org_reveal_presentation/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
]
});
</script>
</body>
</html>
